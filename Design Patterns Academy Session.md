#Day 1#
* ODS (Operational Data Store) is like a Staging DB before you push to WareHouse. We can do cleansing, Integration etc mostly technical operations on data and mostly we dont handle any business logic here. 
* ODS will be Normalized just like OLTP. We can Also generate Operational Reports from ODS. 
* OLTP to STG to ODS to DW to DM - this is Kimball approach. 
* INMON top down approach OR Kimball Bottom up Approach. Kimball is popular. 
* Why do we need STG DB? because generally we dont have access to OLTP for much time (because they want OLTP to be fast and stable). So may be 1 hr per day , we will get a chance to accessit. During that time we should just pull data and Store data in STG db and do further actions on that data to move it to warehouse later. So, this kind of Loading into staging first is nothing but ELT (Load first and transform later). Check what exactly is ELT in google. 
* ODS is needed if you have ER modelling in your warehouse. But if you use Dimensional Modeling and have proper Audit Columns in the FACT table, then We dont need ODS for Operational Reports. We can get them from DW itself. 
* INMON suggests ER model. Kimball suggests Dimensional Model. Query Performance will be better in kimball model. 
* MDM (Meta Data Repository) this holds Metadata for all DB systems in the BI Architechture. Some people maintain it in Excel files (Data Dictionary) which is not Reliable. 
* WHat has happened, Why it happened, How it happened?, what will happen, How to improve what will happen - These are five questions that a good BI System should be able to answer. 
* One draw back of Kimball Bottom Up Approach is that integration is difficult. THere is also a hybrid approach where in the Design phase we follow Top down Approach (Inmon) and then for the rest of phases (Dev, testing, UAT etc) we follow Kimball Approach (Bottom Up).
* Conformed dimension : is a common dimension on various departments in Bus Matrix. Example Time Dimension. 
* Dimensions are also called as subjects in Kimbal dimensional Model. and departments are nothing but processess. 
* Data Profiling should be done by person who understands DATA. 
* Kimbal suggests Normalized Fact (With most detailed level of grain) and De-normalized Dimension (which means clubbing related dimensions(Subjects) to a single Dimension Table).
* Data-Vault Modelling is a latest modelling design in which is completely flexible (with respect to adding or removing dimensions). drawbacks of this are, it is very complex to implement and also BI tools cannot directly access it as of now. So, in the current models, Star-Schema (kimball Dimensional Model) is a flexible model in which removing dimenison is good but adding dimension is still not really very flexible.
* SCD : Actually Type1 means no history and hence by definition of DW, Type 1 should not be in DW. So, generally we should avoid Type1. Type2 maintains Complete History. Type 3 is partial History. Kimball does not suggest Type 3. Best Approach for SCD is Type 2. 
* One reason for why people go to Type3 Scd is if we have Curr and Prev entry in the same row, when we are querying for a report which compares current previous data, we can do it with single query instead of multiple query (if Type2 were used) and hence performance will be better.
* Modelling - Conceptual - Logical - Physical Models. Conceptual comes from Client input. Logical model will be done by Modeller based on Conceptual Model by adding Attributes, Dimensions, Facts. Importance of Conceptual Model is it is a quick way to ensure that we and client are on the same page. If we directly start with Logical Model, THe client will be confused with the model since it is technical and complex and also we need lot of time to build Logical model and if what ever we built is not correct then all the time we spent is waste. 
* Data Stewardship : intorduced by Inmon. THis for securing the Data structure and Data as per business rules and one person (Data Steward) will have access to impose such constraints or Keys and if any one wants to make any changes, they need to contact him. 
* ELT - Oracle Data Integrator (ODI) uses ELT method. Informatica uses ETL method. 
* Opertaional Reports (Can be taken from OLTP Generally), Tactical Reports (Short term Strategic reports) and Strategic reports (Long term Stractegic reports).
* The Name DASHBOARD for BI is actually taken from CAR Dashboard. It actually means, Like the Dashboard in car, All the items that the user needs are placed on the dashboard (like Tabs in qlikview) and he can select what ever he wants. This is benefecial from traditional hierarchial approach because hierarchieal reports frustate the user and this causes the user to not use the bi tool itself (Imagine how frustated you will be if the items on the car dashboard are in hierarchieal manner).
* SUbject-Oriented, Non-Volatile, Time Variant and Integrated. These are the four pillars of Datawarehouse. OLTP is a process oriented design. DWH is a Subject oriented Design. 
* DATA MART is a Datawarehouse for a Specific Domain/process or a set of closely related processes. Whether it is DWH or Datamart, Dimensional Model is the best way to go if performance is the key. Two Types of Datamarts : Dependent and Independent. Kimbal Approach is Bottomup which means, build Datamart first and then warehouse. Inmon Approach is Topdown Approach, that  is to build DWH first and then Data Marts. Inmon Approach leads to Dependent Datamart. Kimball approach leads to Independent Datamarts.
* ODS is mostly used for B2B Transactions.. WHy?

#Day 2#
* Federated Datawarehouse: When companies have different Databases (silos) indifferent countries. These silos have different structure and not very easy to integrate. But we need to integrate them so that the Head office can get an over all picture of the Orignization. So the Organization will have a Federated Dataware house which will have Organization wide Dimension tables and these dimensions will be mapped with Dimensions of Silos. Two types of federated Datawarehouses (integrating Org DW with silos and intergrating Silos with Org DW- check google). Also, Federated Datawarehouse is a practical alternative to MDM (Master Data Management), which is complex.
* Data Modelling : E-R Model (Inmon- Entity-Relation) , Dimensional Model(Kimball- DIM-FACT), Data Vault Model (HUB-SPOKE,Still work in progress).
* Documents we need as Data Modeller : Conceptual Model, Logical Model, Physical Model.
* Master Entities in ER modelling is like DIMENSION. Reference Master Entity in ER is like an Attribute in DIM Modelling. 
* Attributes are like denormalized dimensions. Because they dont have a seperate Dimension table and they will be placed in a related dimension table in a denormalized manner. 
* Factless Fact table -> Check Google. it is a table which combines hierarchieal Dimensions so that we dont have toomany dimensions in star schema?
* Logical Model and Physical Model are NOT the same. Logical Model will not have technical details. Example, Customer and Location dimensions have a many-to-many relashionship but RDBMS doesnot accept many to many so we have have to go with Additional table (Think ER modelling). but we should not put this Additional table in Logical Model because Logical model will be discussed with business user and he will just tell that many-to-many rdbms is your problem and why are to telling it to me. So logica model is more of business model of subjects (dimensions) and Physical model is a technical model of subjects.
* ER-Modelling is best suited for OLTP systems because it is process oreiented (instead of subject oriented) and it has no redundancy in data and hence inserts and updates will be faster. But read operation will be slower here because there are many joins involved (due to normalization). ER-Modelling has two kinds to tables MASTER (MASTER and MASTER REFERENCE) and TRANSACTION. 
* In ER modelling, if we have Many-to-Many relationship, we need seperate table. Example for Product and Customer we need a seperate table called ProductCustomer which marks the many to many relation between the two entities. One-to-one relationships will be very small in an organization. A small example for one-to-one is Person and Voter. 
* In ER if we have 5 master entities which have many to many relationships among themselves, we need to create 4 additional tables (as discussed in above point). Remember that we cant have just one table for tracking all many-to-many relationships here because that table will be de-normalized and in ER we dont have de-normallized table.
* Avoid loops while you try to do relationships in ER modelling. People get confused and do loops trying to relate all possible entities. 
* Check what is 1st normalform, 2nd , 3rd etc in ER modelling in Google. 
* In Dimensional Modellling, to define hierarchy in Dimensions, We split dimension tables forming a STAR-FLAKE formations (only serial links and not V shaped links). If there are multi Splits (v-shaped splits), we call it a SNOW_FLAKE. (Check Star Flake and Snow flake in google). The name of STAR schema is not because it looks like a star. Instead, it is because it is derived from SOlar System where sun (Fact) is at center and planets (Dimensions) are surrounding it.
* CUBE (in Dimensional Modelling) is a logical Structure. Physically it doesnot exist. A Cube is a Fact Table associated with Dimensions. According to inmon, one cube must have only one Master ( or multiple master?) and Transaction Tables. According to Kimball, Cube can have Fact and multiple Dimensions.
* Multi-Star Schema comes whem Two Star Schemas Share some of the Dimensions (Conformed dimensions).
* Tree types of measures (Fully additive: Example Revenue, Semi-Additive: quantity (customer Buys 2 apples and 1 liter milk. We cannot just add up the Quantity), non-Additive: Example Discount% or any % values. Rate is not additive. if we buy 2 products, the rate of product cannot be double or in business terms we are not looking for sum of rates)
* WHat is HYBRID or EXTENTED SNOW FLAKE schema? Check Google.
* Extented Star schema has Multiple Dimension tables for same subject. THis is the difference between Extended start schema and star schema.
* Galaxy Model/Mutli-Star Schema is nothing but Two Star Schemas Combined with the Help of Conformed Dimensions. This is a very common model used in enterprises
* Data-Vault Modelling: Can be used only for DW and not for OLTP. This model came up as a result of difficulty in adding any new dimension to an existing Fact Table. This is a completely flexible model if we want to add anything to existing warehouse.
* HUB, LINK and Satellite are present here. DIMENSIONS(and Facts?) will go to HUB. ATTRIBUTES and other Dimension Detail will go to SATELLITE. Various HUBS will be connected by using a LINK TABLE. 'RecordSource' and Timestamp are common across all tables. Where will FACT Data go ? Check Google. BI Tools cannot use Data Vault model. We need to build a dimensional model on top of it for that. So, What is the actual use of Data Vault model? It is to Store Historic Data with a proper structure so that if in the future we need to add a new dimension in the dimensional model that is built on top of Vault, we can get all historic information and load all the historic Fact data including for the New dimension with out any problem. Check this in google. When ever you think of ODS, Go for DAtaVault instead. -- Check why. Inmon recommends Datavault model for Datawarehouse. The number of Tables in Datavault model will be significantly higher than ER and DIM Models.  DataVault is actually a Physical Model and not a Logical or Conceptual Model. If you try to put Datavault model into Conceptual Model, End user will not undestand it at all. 
* ETL Process: Kimball Calls ETL a DataWarehouse Back Room. 
* Online Extraction: directly connect to OLTP and do an Full/Incremental Extraction. Offline Extraction: OLTP System will not be accessible. Then OLTP team will give incremental dumps (like txt files) of OLTP Data. We then extract from those dumps. other ways are also there for offline extraction like from Redo & Archive logs Or from Snapshots but they are less common and more complex than dumps.
* Online Extraction has obvious advantage over offline in terms of data quality since if there is any error in the source, we have to wait a full day for getting correct data. 
* For a DW, Always gor for Incremental. TYpe2 and Full Load dont go together. If we do Full load we cant do Type 2 and viceversa. 
* Deletion is not an option in DW. but, if client says i need only 20 years of history and he does not want old data. If the data deletion is required, we should not do it in ETL Tool. Instead use a backend process (SP etc) and do the Deletion. ETL is not for Deletion. Also it is not recommended to Do the Deletion on Dimension tables because anyway they dont occupy much space. So, we can delete the data in Fact and not in Dimension. 
* CDC (to Capture incremental data from OLTP) : can be done using Timestamp Method (Capture insert or Lastupadte records > Last Load Date From Source), partition method (partitions based on Time)  and Trigger Method (Trigger on insert or update in OLTP).
* Transformation Methods : MultiStage Tranformation (STG-CIF-ODS like we did in UHG) and PIPELINE Transformation (One shot load for all transformations). In Pipeline, ALl transformations are done in single shot. this will be faster. But in multiStage will be good for Debugging or Understanding?
* Load types: Bulk load (will not create a log?), Normal, Incremental, Full, Sequential (Dimension and the fact).
* If all the Data tables are in the same server, DB, Disc and File, then performance will be slow. IF we want faster performace, we can place table that are to be loaded parallelly (May be two fact tables) on different discs or Files then load performance will improve. 
* Server Architechture: SMP and MPP architechture. Check Google. Hadoop is Based on MPP. For normal projects SMP is suggest. But for Big Data Projects , MPP is preferred. 
* Basic forms of Network Storage : DAS, NAS, SAN. SAN is widely used. -- Check Google. 
* Before Data load, Disable Indexes/Constraints and Enable the after load. 
* Define Proper Check points For Large data loads to recover from the Failure point rather than starting the whole process again. -- Check this in google. 
* External Trainer Email id : abdulraheem26@gmail.com

End of Session.
