# SQL SERVER 2012 Query Performance Tuning #
**Current Status - Page No : 105** 
* if you want to read a book on database design with an emphasis on introducing the subject, I recommend reading
Pro SQL Server 2008 Relational Database Design and Implementation by Louis Davidson et al (Apress, 2008). 
* read the Microsoft white paper “SQL Server 2005 Waits and Queues” http://download.microsoft.com/download/4/7/a/47a548b9-249e-484c-abd7-29f31282b04d/Performance_Tuning_Waits_Queues.doc
* when it comes to wait types, Bob Ward’s repository (collected at http://blogs.msdn.com/b/psssql/archive/2009/11/03/the-sql-server-wait-type-repository.aspx  is a must read.
* For additional information on how to create counter logs using Performance Monitor, please refer to the Microsoft Knowledge Base article “Performance Tuning Guidelines for Windows Server 2008” at http://download.microsoft.com/download/9/c/5/9c5b2167-8017-4bae-9fde-d599bac8184a/Perf-tun-srv.docx
* Did SQL Provider Make way for Extended Events GUI in Sql Server 2012??
* 

##Chapter 1: SQL Query Performance Tuning##
* Even an application running as a service can consume a good part of the system resources and limit the resources available to SQL Server. For example, applications may be configured to work with the processor at a higher priority than SQL Server. Priority is the weight given to a resource that pushes the processor to give it greater preference when executing.
* SQL Server process (sqlservr.exe) by default runs at Normal priority, whereas the Windows Task Manager process (taskmgr.exe) runs at High priority. Therefore, to allow SQL Server to maximize the use of available resources, you should look for all the nonessential applications/services running on the SQL Server machine and ensure that they are not acting as resource hogs.
* You should also look at the configuration of SQL Server, since proper configuration is essential for an optimized application. There is a long list of SQL Server configurations that defines the generic behavior of a SQL Server installation. These configurations can be viewed and modified using a system stored procedure, sp_configure. Many of these configurations can be managed interactively through SQL Server Management Studio.
* When processes run on a server, even one with multiple processors, at times one process will be waiting on another to complete. You can get a fundamental understanding of the root cause of slowdowns by identifying what is waiting and what is causing it to wait. You can realize this through operating system counters that you access through dynamic management views within SQL Server.
* Performance tuning is an iterative process where you identify major bottlenecks, attempt to resolve them, measure the impact of your changes, and return to the first step until performance is acceptable. When applying your solutions, you should follow the golden rule of making only one change at a time where possible.Any change usually affects other parts of the system, so you must reevaluate the effect of each change on the performance of the overall system.
* Instead of tuning a system to the theoretical maximum performance, the goal should be to tune until the system performance is “good enough.” This is a commonly adopted performance tuning approach. The cost investment after such a point usually increases exponentially in comparison to the performance gain. The 80:20 rule works very well: by investing 20 percent of your resources, you may get 80 percent of the possible performance enhancement, but for the remaining 20 percent possible performance gain, you may have to invest an additional 80 percent of resources.

##Chapter 2: System Performance Analysis  ##
* System behavior can be either tracked in real time in the form of graphs or captured as a log (called a data collector set) for offline analysis. The preferred mechanism on production servers is to use the log.To run the Performance Monitor tool, execute perfmon from a command prompt, which will open the Performance Monitor suite.
* To get an immediate snapshot of a large amount of data that was formerly available only in Performance Monitor, SQL Server now offers the same data internally through a set of dynamic management views (DMVs) and dynamic management functions (DMFs) collectively referred to as dynamic management objects (DMOs). These are extremely useful mechanisms for capturing a snapshot of the current performance of your system.
* the 'cntr_type' column in '**sys.dm_os_performance_counters**' DMO is documented here: http://msdn.microsoft.com/en-us/library/aa394569(VS.85).aspx
* **sys.dm_os_wait_stats**. This DMV shows an aggregated view of the threads within SQL Server that are waiting on various resources, collected since the last time SQL Server was started or the counters were reset. One of the most common types of waits is I/O. If you see ASYNCH_I0_C0MPLETI0N, I0_C0MPLETI0N, LOGMGR, WRITELOG, or PAGEIOLATCH in your top ten wait types, you may be experiencing I/O contention, and you now know where to start working.
* Typically, SQL Server database performance is affected by stress on the following hardware resources: Memory, Disk I/O, Processor, Network.
* The most common performance problem is usually I/O, either from memory or from the disk.
* Memory can be a problematic bottleneck because a bottleneck in memory will manifest on other resources, too. This is particularly true for a system running SQL Server. When SQL Server runs out of cache (or memory), a process within SQL Server (called lazy writer) has to work extensively to maintain enough free internal memory pages within SQL Server. This consumes extra CPU cycles and performs additional physical disk I/O to write memory pages back to disk. The good news is that SQL Server 2012 has changed memory management. A single process now manages all memory within SQL Server; this can help to avoid some of the bottlenecks previously encountered because max server memory will be applied to all processes, not just those smaller than 8k in size.
* The performance Counters described in page 30 and their details in subsequent pages, correspond to the DMV sys.dm_os_performance_counters. They can also be added from Performance Monitor (perfmon in Run Command) and monitored.
* **DBCC memorystatus** command also gives you a set of measures of where memory is currently allocated.
* Frequently used DMVs for Memory Bottlenecks : Sys.dm_os_memory_brokers, Sys.dm_os_memory_clerks, Sys.dm_os_ring_buffers
* A few of the common resolutions for memory bottlenecks are as follows: • Optimizing application workload • Allocating more memory to SQL Server • Increasing system memory • Changing from a 32-bit to a 64-bit processor • Enabling 3GB of process space • Data Compression
* To identify which queries are using more memory, query the sys.dm_exec_query_memory_grants DMV. Just be careful when running queries against this DMV using a JOIN or an ORDER BY statement; if your system is already under memory stress, these actions can lead to your query needing its own memory grant.
* In SQL Server 2012, a 32-bit instance of SQL Server is limited to accessing only 3GB of memory. The limitations on SQL
Server for memory go from 3GB to a limit of up to 8TB depending on the version of the operating system and the specific processor type.
* Data compression has a number of excellent benefits for storage and retrieval of information. It has an added benefit that many people aren’t aware of: while compressed information is stored in memory, it remains compressed. This means more information can be moved while using less system memory, increasing your overall memory throughput. All this does come at some cost to the CPU, so you’ll need to keep an eye on that to be sure you’re not just transferring stress.
* SQL Server can take advantage of multiple filegroups by accessing tables and corresponding nonclustered indexes using separate I/O paths.
* SQL Server log files should always, when possible, be located on a separate hard disk drive from all other SQL Server database files. Transaction log activity primarily consists of sequential write I/O, unlike the nonsequential (or random) I/O required for the data files. Separating transaction log activity from other nonsequential disk I/O activity can result in I/O performance improvements because it allows the hard disk drives containing log files to concentrate on sequential I/O.
* The major portion of time required to access data from a hard disk is spent on the physical movement of the disk spindle head to locate the data. Once the data is located, the data is read electronically, which is much faster than the physical movement of the head.
* As a general rule of thumb, you should try, where possible, to isolate files with the highest I/O from other files with high I/O. This will reduce contention on the disks and possibly improve performance. To identify those files using the most I/O, reference sys.dm_io_virtual_file_stats.
* Creating a partition moves the segment of data to a particular filegroup and only that filegroup. This provides a massive increase in speed because, when querying against well-defined partitions, only the files with the partitions of data you’re interested in will be accessed during a given query.Just remember that partitions are primarily a manageability feature.
While you can see some performance benefits from them in certain situations, it shouldn’t be counted on as part of partitioning the data. SQL Server Denali supports up to 15,000 partitions.
* DMOs for CPU Performance: Sys.dm_os_wait_stats, Sys.dm_os_workers and Sys.dm_os_schedulers
* A way to check for missing indexes is to query the dynamic management view sys.dm_db_missing_index_details.
* The opposite problem to a missing index is one that is never used. The DMV sys.dm_db_index_usage_stats shows which indexes have been used, at least since the last reboot of the system. You can also view the indexes in use with a lower-level DMV, sys.dm_db_index_operational_stats. It will help to show where indexes are slowing down because of contention or I/O
* Recompilations of stored procedures add overhead on the processor. You want to see a value close to 0 for the SOL Re-Compilations/sec counter. If you consistently see nonzero values, then you should use Extended Events to further investigate the stored procedures undergoing recompilations

##Chapter 3: SQL Query Performance Analysis##
* Extended Events were introduced in SQL Server 2008, but with no GUI in place and a reasonably complex set of code to set them up, they weren’t used much to capture performance metrics. With SQL Server 2012, a GUI for managing extended events was introduced, making this the preferred mechanism for gathering query performance metrics among other things.
* An RPC event indicates that the stored procedure was executed using the Remote Procedure Call (RPC) mechanism through an OLEDB command. If a database application executes a stored procedure using the T-SQL EXECUTE statement, then that stored procedure is resolved as a SQL batch rather than as an RPC.
* A T-SQL batch is a set of SQL queries that are submitted together to SQL Server. A T-SQL batch is usually terminated by a GO command. The GO command is not a T-SQL statement. Instead, the GO command is recognized by the sqlcmd utility, as well as by Management Studio, and it signals the end of a batch. Each SQL query in the batch is considered a T-SQL statement.
* Statement completion events should be collected judiciously(when we are monitoring using Extended Events), especially on a production system.
* Once a session (of ExtendedEvents using SSMS) is started on the server, you don’t have to keep Management Studio open any more. You can identify the active sessions by using the dynamic management view sys.dm_xe_sessions. You can stop a specific
session by executing the stored procedure ALTER EVENT SESSION. To verify that the session is stopped successfully, reexecute the view sys.dm_xe_sessions, and ensure that the output of the view doesn’t contain the named session
* There are a number of events (in Extended Events) related to debugging SQL Server. These are not available through the Wizard, but you do have access to them through the TSQL command. Do not use them. They are subject to change and are meant for Microsoft internal use only. If you do feel the need to experiment, you need to pay close attention to any of the events that include a break action. This means that should the event fire, it will stop SQL Server at the exact line of code that caused the event to fire. This means your server will be completely offline and in an unknown state. This could lead to a major outage if you were to do it in a production system. It could lead to loss of data and database corruption.
* Setting up an Extended Events session allows you to collect a lot of data for later use, but the collection can be a little bit expensive, you have to wait on the results, and then you have a lot of data to deal with. If you need to immediately capture performance metrics about your system, especially as they pertain to query performance, then the dynamic management views sys.dm_exec_query_stats for queries and sys.dm_exec_procedure_stats for stored procedures are what you need.
* To filter the information returned from sys.dm_exec_query_stats, you’ll need to join it with other dynamic management functions such as sys.dm_exec_sql_text, which shows the query text associated with the plan, or sys.dm_query_plan, which has the execution plan for the query
* You can read directly from the extended events file you can query it using this system function:SELECT * FROM sys.fn_xe_file_target_read_file('C:\Program Files\Microsoft SQL Server\MSSQL11.RANDORI\MSSQL\Log\Query Performance
Tuning*.xel’, NULL, NULL, NULL); 
The query returns each extended event as a single row. The data about the event is stored in an XML column, event_data. You’ll need to use XQuery to go against the data directly, but once you do, you can search, sort and aggregate the data captured
* The costly queries can be categorized into the following two types: • Single execution: An individual execution of the query is costly. • Multiple executions: A query itself may not be costly, but the repeated execution of the query causes pressure on the system resources.
* This (querying DMOs) is so much easier than all the work required to gather session data that it makes you wonder why you
would ever use extended events at all. The main reason is precision. The sys.dm_exec_ query_stats view is a running aggregate for the time that a given plan has been in memory. Ann extended events session, on the other hand, is a historical track for whatever time frame you ran it in. You can even add session results together within a database and have a list of data that you can generate totals in a more precise manner rather than simply relying on a given moment in time. But understand that a lot of troubleshooting of performance problems is focused on that moment in time when the query is running slowly. That’s when sys.dm_exec_query_stats becomes irreplaceably useful.
* The two types of execution plan are the estimated plan and the actual plan. The estimated plan represents the results coming from the query optimizer, and the actual plan is the plan used by the query engine. The beauty of the estimated plan is that it doesn’t require the query to be executed. The plans generated by these types can differ, but most of the time they
will be the same. The primary difference is the inclusion of some execution statistics in the actual plan that are not present in the estimated plan.
* The thickness of a connecting arrow between operators (in an Execution Plan) represents a graphical representation of the number of rows transferred. Observe the thickness of the connecting arrows between nodes. A very thick connecting arrow indicates a large number of rows being transferred between the corresponding nodes. Analyze the node to the left of the arrow to understand why it requires so many rows. Check the properties of the arrows too. You may see that the estimated rows and
the actual rows are different. This can be caused by out-of-date statistics, among other things.
* There may be warnings, indicated by an exclamation point on one of the operators (in an Execution Plan), which are areas of immediate concern. These can be caused by a variety of issues, including a join without join criteria or an index or a table with missing statistics. Usually resolving the warning situation will help performance.
* Watch for extra operators that may be placing additional load on the system such as table spools (in an Execution Plan). These may be necessary for the operation of the query, or they may be indications of an improperly written query or badly designed indexes.
* The default cost threshold for parallel query execution is an estimated cost of 5 and that’s very low. Watch for parallel operations where they are not warranted. (** Whats this? in ExecutionPlan**)
* Usually, for best performance, you should retrieve as few rows as possible from a table, and an index seek is usually the most efficient way of accessing a small number of rows. A scan operation usually indicates that a larger number of rows
have been accessed. Therefore, it is generally preferable to seek rather than scan.
* A hash join uses the two join inputs as a build input and a probe input. The build input is shown as the top input in the execution plan, and the probe input is shown as the bottom input. The smaller of the two inputs serves as the build input.
The hash join performs its operation in two phases: the build phase and the probe phase. In the most commonly used form of hash join, the 'in-memory hash join', the entire build input is scanned or computed, and then a hash table is built in memory. Each row is inserted into a hash bucket depending on the hash value computed for the hash key (the set of columns in the equality predicate). This build phase is followed by the probe phase. The entire probe input is scanned or computed one row at a time, and for each probe row, a hash key value is computed. The corresponding hash bucket is scanned for the hash key value from the probe input, and the matches are produced.
* A merge join requires both join inputs to be sorted on the merge columns, as defined by the join criterion. If indexes are available on both joining columns, then the join inputs are sorted by the index. Since each join input is sorted, the merge join gets a row from each input and compares them for equality. A matching row is produced if they are equal. This process is
repeated until all rows are processed.In situations where the data is ordered by an index, a merge join can be one of the fastest join operations (faster than Hash Join and Nested Loop), but if the data is not ordered and the optimizer still chooses to perform a merge join, then the data has to be ordered by an extra operation. This can make the merge join slower and more costly in terms of memory and I/O resources.
* A nested loop join uses one join input as the outer input table and the other as the inner input table. The outer input table is shown as the top input in the execution plan, and the inner input table is shown as the bottom input table. The outer loop consumes the outer input table row by row. The inner loop, executed for each outer row, searches for matching rows in the inner input table. Nested loop joins are highly effective if the outer input is quite small and the inner input is large but indexed. In many simple queries affecting a small set of rows, nested loop joins are far superior to both hash and merge joins. Joins operate by gaining speed through other sacrifices. A loop join can be fast because it uses memory to take a small set of data and compare it quickly to a second set of data. A merge join similarly uses memory and a bit of tempdb to do its ordered comparisons. A hash join uses memory and tempdb to build out the hash tables for the join. Although a loop join is faster, it will consume more memory than a hash or merge as the data sets get larger, which is why SQL Server will use different plans in different situations for different sets of data.
* Refer to table 3-7 in page 97
* Some times you cannot see Estimated pLans (for example, if there is a procedure which creates and object and Drops it, Execution plan will not work on that SP). You can only get actual Execution Plan there. 
* One final place to access execution plans is to read them directly from the memory space where they are stored, the 'plan cache'. Dynamic management views and functions are provided from SQL Server to access this data. To see a listing of execution plans in cache, run the following query:
SELECT p.query_plan,
t.text
FROM sys.dm_exec_cached_plans r
CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) p
CROSS APPLY sys.dm_exec_sql_text(r.plan_handle) t ;
* Even though the execution plan for a query provides a detailed processing strategy and the estimated relative costs of the individual steps involved, it doesn’t provide the actual cost of the query in terms of CPU usage, reads/writes to disk, or query duration. While optimizing a query, you may add an index to reduce the relative cost of a step. This may adversely affect a dependent step in the execution plan, or sometimes it may even modify the execution plan itself. Thus, if you look only at the execution plan, you can’t be sure that your query optimization benefits the query as a whole, as opposed to that one step in the execution plan. You should analyze the overall cost of a query in different ways(like using Extended Events).
* **Client statistics** capture execution information from the perspective of your machine as a client of the server. This means that any times recorded include the time it takes to transfer data across the network, not merely the time involved on the SQL Server machine itself. To use them, simply click Query  Include Client Statistics.
* Although capturing client statistics can be a useful way to gather data, it’s a limited set of data, and there is no way to show how one execution is different from another. You could even run a completely different query, and its data would be mixed in with the others, making the averages useless. If you need to, you can reset the client statistics. Select the Query menu and then the Reset Client Statistics menu item.
* we can clear out the cache using the system call DBCC FREEPROCCACHE (resulting in clearing Execution plans from plan cache). you should not run DBCC FREEPROCCACHE on your production systems unless you are prepared to incur the not insignificant cost of recompiling every query on the system. in some ways, this will be as costly to your system as a reboot or a SQL Server instance restart. 
* The number of reads in the 'Reads' column is frequently the most significant cost factor among 'duration', 'cpu', 'reads' and 'writes'(in Statics io). The total number of reads performed by a query consists of the sum of the number of reads performed on all tables involved in the query. The reads performed on the individual tables may vary significantly, depending on the size of the result set requested from the individual table and the indexes available.
* During optimization steps, you need a nonfluctuating cost figure as a reference. The reads (or logical reads) don’t vary between multiple executions of a query with a fixed table schema and data. For example, if you execute a SELECT statement hundred times, you will probably get hundred different figures for duration and CPU, but Reads will remain the same each time. Therefore, during optimization, you can refer to the number of reads for an individual table to ensure that you really have reduced the data 'access' cost of the table.
* Even though the number of logical reads can also be obtained from the Extended Events, you get another benefit when using STATISTICS IO. The number of logical reads for a query shown by Profiler or the Server Trace option increases as you use different SET statements (like STATISTICS) along with the query. But the number of logical reads shown by STATISTICS IO doesn’t include the additional pages that are accessed as SET statements are used with a query. Thus, STATISTICS IO provides a consistent figure for the number of logical reads.

##Chapter 4: Index Analysis##
* 
